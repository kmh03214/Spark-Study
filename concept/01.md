# 아파치 스파크

- 통합 컴퓨팅 엔진 
    - 클러스터환경에서 데이터를 병렬로 처리하는 라이브러리 집합
- 언어지원
    - 파이썬 / 자바 / 스칼라 / R
    - 스칼라로 만들어졌고, 이후로 자바에서 개발됨
- 실행환경
    - 단일 노트북 환경에서, 수천 대의 서버로 구성된 클러스터까지 다양한 환경에서 실행 할 수 있다. 따라서, 빅데이터 처리에 용이하고 엄청난 규모의 클러스터로 확장 가능


## 1.1 아파치 스파크 철학

- 통합(unified)
    간단한 데이터 읽기부터 SQL 처리, 머신러닝, 스트림 처리에 이르기까지 다양한 데이터 분석 작업을 같은 연산엔진과 일관성 있는 API로 수행할 수 있도록 설계됨

    - 빅데이터 application 개발에 필요한 통합 플랫폼을 제공하자는 목표가 있음
    왜냐하면, 데이터 분석 작업이 다양한 처리 유형과 라이브러리를 결합해 수행되기 때문, 이러한 라이브러리를 유연하게 쓸 수 있게 만들어줌.

    ex) SQL 쿼리로 데이터를 읽고 ML라이브러리로 머신러닝 모델을 평가해야 할 경우 스파크 엔진은 이 두 단계를 하나로 병합하고 데이터를 한번만 조회할 수 있게 해준다.


- 컴퓨팅 엔진
    통합이라는 관점에 의해 기능의 범위를 컴퓨팅 엔진으로 제한함. 따라서, 스파크는 저장소 시스템의 [데이터를 연산] 하는 역할만 수행할 뿐 영구 저장소 역할은 수행하지 않고, 클라우드기반의 Azure Storage / S3 등의 저장소를 지원함

    데이터 이동은 많은 비용이 발생하기 때문에, 데이터 저장 위치에 상관없이 처리에 집중하도록 만들어 짐.

    - 스피크는 연산기능에 초점을 맞춤
        아파치 하둡같은 기존 빅데이터 플랫폼과 차별화된다. [저장공간 + 컴퓨팅 시스템(맵리듀스)]

        스파크는, 하둡저장소와 잘 호환되고, 연산노드와 저장소를 별도로 구매할 수 있는 공개형 클라우드 환경이나 스트리밍 애플리케이션이 필요한 환경에서도 사용가능 하기에 강점이 있음.

- 라이브러리
    데이터 분석 작업에 필요한 통합 API를 제공
    
    기본 라이브러리 외에 다양한 저장소 시스템을 위한 connector부터 머신러닝을 위한 알고리즘등 수백 개의 외부 오픈소스 라이브러리 존재


## 1.2 스파크의 등장배경

데이터 분석에 새로운 처리 엔진과 프로그래밍 모델이 왜 필요했을까?

-> 컴퓨터 애플리케이션과 하드웨어의 경제적요인(비용/발전속도) 떄문임

컴퓨터는 프로세서 성능 향상으로 해마다 빨라졌음. 대부분 시스템은 단일 프로세서에만 실행되도록 설계됨.

하지만, 물리적인 방열 한계로 단일 프로세서의 성능을 향상시키기가 어려워짐(2005년)

따라서, 모든 코어가 같은 속도로 동작하는 병렬 CPU코어를 더 많이 추가하는 방향으로 발전함 (코어의 갯수 증가)

즉, 애플리케이션의 성능 향상을 위해서는 병렬처리가 필요하고 스파크 같은 프로그래밍 모델의 시초가 됨

데이터의 수집 비용은 낮아지고 데이터가 많아지면서 처리문제가 생김
프로세서의 성능 향상 -> 병렬처리로 추세 변화

## 스파크 실행

1.	자바설치
$ brew tap AdoptOpenJDK/openjdk
$ brew cask install adoptopenjdk8

OpenJDK 버전은 다음중 하나를 선택하면 된다.

OpenJDK8 - adoptopenjdk8
OpenJDK9 - adoptopenjdk9
OpenJDK10 - adoptopenjdk10
OpenJDK11 - adoptopenjdk11
OpenJDK11 w/ OpenJ9 JVM - adoptopenjdk11-openj9

다운로드가 100% 완료되면 패스워드를 치는 창이 나타나고 패스워드 입력 후 설치가 완료된다.

[설치확인]
설치확인 (환경변수설정이 되어있음)

$ java -version
$ /usr/libexec/java_home

2. Scala 설치
$ brew install scala
$ scala --version

3. Spark 설치
$ brew install apache-spark
설치 후, 다음 명령어로 spark가 제대로 실행되는지 확인 할 수 있다.
$ spark-shell

4. pyspark는 spark 설치시 자동으로 설치된다.
다음 명령어를 통해 실행 확인
$ pyspark

5. jupyterlab을 설치하면 jupyter 환경에서 pysaprk을 사용할 수 있다.
$ brew install jupyterlab

다음으로 .bashrc나 .zshrc에 pyspark 환경변수 설정(추가)을 한다

export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

pyspark 실행시 jupyterlab이 바로 실행 된다.





